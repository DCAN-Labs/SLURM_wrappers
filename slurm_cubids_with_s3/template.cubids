#!/bin/bash

subj_id=SUBJECTID
ses_id=SESID
data_dir=DATADIR
data_bucket=BUCKET
run_dir=RUNDIR
cpu_usage=8
singularity=`which singularity`
output_csv=OUTPUTCSV

HASH=`tr -cd '[:alnum:]' < /dev/urandom | fold -w8 | head -n1`

# pull down needed data and files from BIDS bucket
if [ ! -d ${data_dir}/sub-${subj_id}/ses-${ses_id} ]; then
	mkdir -p ${data_dir}/${HASH}/sub-${subj_id}
	s3cmd get ${data_bucket}/niftis/sub-${subj_id}/ses-${ses_id} ${data_dir}/${HASH}/sub-${subj_id}/ --recursive -v
fi
if [ ! -e ${data_dir}/dataset_description.json ]; then
	cp ${run_dir}/dataset_description.json ${data_dir}/${HASH}
fi


source /home/faird/shared/code/external/envs/miniconda3/load_miniconda3.sh
conda activate cubids

cubids-validate ${data_dir}/${HASH}/ ${data_dir}/${HASH}/sub-${subj_id}_ses-${ses_id}

one_subject_csv=${data_dir}/${HASH}/sub-${subj_id}_ses-${ses_id}_validation.tsv

flock -x ${output_csv} cat ${one_subject_csv} | tail -n +2 >> ${output_csv}

rm -r ${data_dir}/${HASH}/


