#!/bin/bash

subj_id=SUBJECTID
ses_id=SESID
data_dir=DATADIR
data_bucket=BUCKET
run_dir=RUNDIR
singularity=`which singularity`

BIDS_DIR=${data_dir}/sub-${subj_id}
OUTPUT_DIR=${data_dir}/processed/dcan-infant-pipeline

# pull down needed data and files from BIDS bucket
if [ ! -d ${data_dir}/sub-${subj_id}/ses-${ses_id} ]; then
	mkdir -p ${BIDS_DIR}
	s3cmd get ${data_bucket}/processed_nnUNet/dcan-infant-pipeline/sub-${subj_id}/ses-${ses_id} ${BIDS_DIR} --recursive -v
fi
if [ ! -e ${data_dir}/dataset_description.json ]; then
	cp ${run_dir}/dataset_description.json ${data_dir}
fi
if [ ! -e ${data_dir}/participants.tsv ]; then
	s3cmd get ${data_bucket}/niftis/participants.tsv ${data_dir} -v 
fi

# create processed and derivatives folders if they do not exist
#if [ ! -d ${data_dir}/processed/dcan-infant-pipeline ]; then
#	mkdir ${data_dir}/processed_nnUNet/
#	mkdir -p ${data_dir}/processed_nnUNet/dcan-infant-pipeline/
#  s3cmd get ${data_bucket}/processed_nnUNet/dcan-infant-pipeline/sub-${subj_id}/ses-${ses_id} ${data_dir}/processed_nnUNet/dcan-infant-pipeline/sub-${subj_id} --recursive -v
#fi

python3 /home/faird/shared/code/internal/utilities/CustomClean/cleaning_script.py \
-j ${run_dir}/HCP-D_BIDS_cleaning.json \
-d ${BIDS_DIR}/ses-${ses_id}

#push processed outputs to bucket
s3cmd sync -F --recursive -v ${BIDS_DIR}/sub-${subj_id}/ses-${ses_id}/ \
${data_bucket}/processed/dcan-infant-pipeline/sub-${subj_id}/ses-${ses_id}/

# run filemapper
