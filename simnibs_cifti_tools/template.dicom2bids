#!/bin/bash
input_dir=DATA_DIR
subject=SUBJECT_ID
session=SESSION_ID
coordinates=COORD
temp_dir=TEMP


#grab data from tier 1 here into temp dir:
cp /home/miran045/shared/projects/Amal_neuromodulation/Experiments/oscar_testing_efield_generator/outputs/sub-MSC01/ses-func01/FEM #copy inside output dir

path_mri_processed_data="/home/miran045/shared/data/MSC_to_DCAN/sub-$subject/ses-$session" #need to copy this into temp code dir
full_path_to_simnibs_cifti_tools="/home/faird/shared/code/internal/pipelines/simnibs_cifti_tools/" #need to copy this into each temp dir
full_path_to_simnibs_container="/home/faird/shared/code/internal/pipelines/container_simnibs/sing_test_simnibs_alone_debian.sif" #need to copy this into each temp dir

HASH=`tr -cd '[:alnum:]' < /dev/urandom | fold -w8 | head -n1`
output_dir='$temp_dir/sub-$subject/ses-$session/$HASH/'

code_dir= $temp_dir/$HASH/code/
input_dir=$temp_dir/$HASH/input/

pwd; hostname; date

python $full_path_to_simnibs_cifti_tools/efield_generator/efield_generator.py -XYZ_atlas $coordinates -path_mri_processed_data $path_mri_processed_data \
-output_folder $temp_dir/$HASH/sub-$subject/ses-$session/ \
-full_path_to_simnibs_container $full_path_to_simnibs_container \
-full_path_to_simnibs_cifti_tools $full_path_to_simnibs_cifti_tools


#sync at subject dir
s3cmd sync -F --recursive -v --delete-removed --force $output_dir/ ${data_bucket}/
#might need multiple commands if you're wanting to be selective
#make all dirs temp, including output
