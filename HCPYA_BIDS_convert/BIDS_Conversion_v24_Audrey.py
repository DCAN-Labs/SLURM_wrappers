'''
This script was written by Nick Souter (N.Souter@sussex.ac.uk) in order to convert raw unprocessed data from the HCP young
adult dataset into BIDS format, for the purpose of running fMRIPrep. TThis script is written such that it works with the file
tree generated by Audrey Houghton, with unprocessed scanning protocols subdivided by subject ID and then session (3T/7T). It
should convert all structural, functional, fieldmap, and diffusion scans. The exception is the second T1/T2 scan possessed by some
participants. At present, only one run of each scan is converted.

Upon running the script, you will have the option to 'copy' files into new BIDS files, or to symbolic 'link' them with new names but
without creating new files. Note that the resuling 'link' directory will not be recognised as BIDS valid, but the 'copy'
version will.

As it stands, the output directory is set to be the current working directory holding this script. Both the input_dir and output_dir
variables should be updated as needed.

It also expects a CSV file containing the participant ID, age, and gender of all participants being converted. If this isn't
present, it'll print a warning rather than crashing.

This script has been written and tested in Linnux. Errors may be encountered if attempting to run or edit on Windows or Mac.
'''

#Imports relevant modules.
import os
import json
import shutil
import glob
import csv
import operator
import nibabel as nib
import numpy as np

#Asks the user to indicate (in the terminal) whether they would like to copy or link the input files.
#Copying will duplicate the files, linking will just create symbolic links. Either 'copy' or 'link'
#(without apostrophes) should be typed. If not, user will be prompted until they enter a valid option.
'''
while True:
	method = input("Please indicate whether you would like to 'copy' or 'link' files:  ").lower()
	if method == 'copy':
		break
	elif method == 'link':
		break
	else:
		print("""***Please type either 'copy' or 'link'. Copying will create new files,
linking will create symbolic links without duplicating NIFTIs.***""")
'''
method='copy'
#Paths to the overall input and output directories are defined. Ouput is defined based on whether copying or linking.
input_dir = os.path.join('tmp','HCP_YA')
output_dir = os.path.join('tmp','output')

#Checks whether the output directory exists. If not, it's created.
if os.path.exists(output_dir):
	pass
else:
	os.mkdir(output_dir)

#Creates a dictionary to store dataset description information.
dataset_description = {'Name': 'HCP Data for fMRIPrep', 'BIDSVersion': '1.8.0',
'DatasetType': 'raw', 'Authors': ['Nicholas E Souter', 'Assisted by hcp2bids']}

#Defines a filename for this description, then writes and creates a corresponding json file.
description_filename = os.path.join(output_dir, 'dataset_description.json')

with open(description_filename, 'w') as f:
	json.dump(dataset_description, f)

#Defines and opens a README file with text defined below.
readme_filename = os.path.join(output_dir, 'README')

with open(readme_filename, 'w') as f:
	f.write("""This data is taken from the HCP young adult dataset. It has been covnerted into BIDS
format for procesisng in fMRIPrep. The conversion script used was generated by Nick Souter for use by Audrey Houghton,
based on a custom input file tree structure created by Audrey.""")

#Checks whether there is a csv containing participant demographics in the current directory.
if os.path.exists('Participant_Demographics.csv'):

	#If so, it's opened and read. An output TSV file is created, and each row from the CSV file is written into it.
	with open('Participant_Demographics.csv', 'r') as demographics:	

		open_demo = csv.reader(demographics)

		with open(os.path.join(output_dir, 'participants.tsv'), 'w') as participants_tsv:
			writer = csv.writer(participants_tsv, delimiter = '\t')

			#While doing so, the prefix 'sub-' must be added to each participant ID. The header is skipped.
			for row in open_demo:
				if row[0] != 'participant_id':
					row[0] = 'sub-' + str(row[0])
				writer.writerow(row)

	#An accompanying json file is defined and created, to describe what each of the demographics headers correspond to.
	participants_json_input = {'age': {'Description': 'Age of the participant. This is in a 5 year range, as exact age is restricted in the HCP',
	'Units': 'years'}, 'sex': {'Description': 'Sex of the participant as reported by the participant', 'Levels': {'M': 'Male', 'F': 'Female'}}}

	participants_json = os.path.join(output_dir, 'participants.json')

	with open(participants_json, 'w') as f:
		json.dump(participants_json_input, f)

#If not, a warning is printed that this file has not been generated/updated, and the script continues.
else:
	print("""The file 'Participant_Demographics.csv' was not found in this directory.
 'participants.tsv' has not been generated/updated.""")

#3T and 7T sessions rely on different phase encoding directions. This dictionary will allow us to reference them later.
phase_dirs = {'ses-3T': {'LR': 'i', 'RL': 'i-'}, 'ses-7T': {'PA': 'j', 'AP': 'j-'}}

#This function is set up to create a copy or symbolic link for a given file. It takes in (a) the file a link should be made to,
#(b) the subfolder that the data corresponds to (e.g., 'anat'), and (c) the name of the resulting output link/file.
def BIDS_convert(target_file, output_category, output_copy):
    
	#The filepath of the original and copy files are defined using variables that will be introduced
	#below, and the subject's ID and session.
	target_file_path = os.path.join(target_data, ('{}_{}_'.format(subject[4:], session[4:]) + target_file + '.nii.gz'))
	copy_path = os.path.join(subject_path, output_category, ('{}_{}_'.format(subject, session) + output_copy + '.nii.gz'))

	#Checks whether the user wants the files copied or linked. Checks if a given link/file exists. If not, the file is linked/copied.
	if method == 'link':
		if os.path.islink(copy_path):
			pass
		else:
			os.symlink(target_file_path, copy_path)
	elif method == 'copy':
		if os.path.exists(copy_path):
			pass
		else:
			shutil.copy(target_file_path, copy_path)

#A seperate function is needed to convert the structural magnitude fieldmaps, as this involves turning
#one 4D file into two 3D files. This is the one case where a symbolic link can't be made, a new files is needed.
#First, takes the input folder as input.
def magnitude_convert():

	#Defines the path that the file should be saved in.
	target_file_path = os.path.join(target_data, '{}_3T_FieldMap_Magnitude.nii.gz'.format(subject[4:]))
	
	#The magnitude file is loaded and opened.
	img = nib.load(target_file_path)
	data = img.get_fdata()

	#Echoes 1 and 2 are defined as the first and second volumes according to the fourth dimension, repsectively.
	echo1 = data[..., :data.shape[-1] // 2]
	echo2 = data[..., data.shape[-1] // 2:]

	#The empty fourth dimension is removed from both files.
	echo1 = np.squeeze(echo1, axis = -1)
	echo2 = np.squeeze(echo2, axis = -1)

	#NIFTI headers are updated such that they are the correct size for the new images.
	header = img.header.copy()
	header['dim'][0] = 3
	header['pixdim'][4] = 1

	#The new output files are defined based on these new arrays, and are then saved out as files.
	echo1_img = nib.Nifti1Image(echo1, img.affine, header)
	echo2_img = nib.Nifti1Image(echo2, img.affine, header)

	nib.save(echo1_img, os.path.join(subject_path, 'fmap', '{}_{}_acq-{}_run-{}_magnitude1.nii.gz'.format(subject, session, scan, run_no)))
	nib.save(echo2_img, os.path.join(subject_path, 'fmap', '{}_{}_acq-{}_run-{}_magnitude2.nii.gz'.format(subject, session, scan, run_no)))

#This function is needed to convert BVEC and BVAL files for DWI scans. Seperate function is needed as these are not NIFTI files.
def dwi_convert():

	#The possible extensions of the files that we're interested in. The loop below iterates through both.
	extensions = ['bval', 'bvec']

	for extension in extensions:
    
		#Defines the input and output filepaths using relevant information, most of which is specified when iterating through subjects below.
		target_file_path = os.path.join(target_data, '{}_{}_DWI_dir{}_{}.{}'.format(subject[4:], session[4:], bvalue, dwi_dir, extension))
		copy_path = os.path.join(subject_path, 'dwi', '{}_{}_acq-dir{}_dir-{}_dwi.{}'.format(subject, session, bvalue, dwi_dir, extension))

		#Checks whether the user wants the files copied or linked. Checks if a given link/file exists. If not, the file is linked/copied.
		if method == 'link':
			if os.path.islink(copy_path):
				pass
			else:
				os.symlink(target_file_path, copy_path)
		elif method == 'copy':
			if os.path.exists(copy_path):
				pass
			else:
				shutil.copy(target_file_path, copy_path)

#This funciton merges multiple dictionaries together (as many as needed). Will be used to generate variants of metadata.
def merge_dicts(*dicts):
	merged = {}
	for metadata in dicts:
		merged.update(metadata)
	return merged

#These dictionaries contain BIDS metadata for the 3T and 7T scanners used. We'll be able to plug this in to all respecive metadata.
hardware_3T = {'Manufacturer': 'Siemens', 'ManufacturersModelName': 'Connectome Skyra', 'MagneticFieldStrength': 3,
'ReceiveCoilName': 'Standard 32-Channel Siemens Receive Head Coil', 'ReceiveCoilActiveElements': 'HEA;HEP'}

hardware_7T = {'Manufacturer': 'Siemens', 'ManufacturersModelName': 'Magnetom', 'MagneticFieldStrength': 7,
'ReceiveCoilName': 'Nova32 32-Channel Siemens Receive Head Coil', 'ReceiveCoilActiveElements': 'A32'}

#Function to attach structural metadata to the appropriate file.
def anat_metadata():

	#Defines the output filename based on the relevant information of the respective NIFTI file.
	metadata_filename = os.path.join(subject_path, 'anat', '{}_{}_run-{}_{}.json'.format(subject, session, run_no, scan))

	#These values apply for both T1 and T2 structural scans.
	constants = {'NonlinearGradientCorrection': 'false','MRAcquisitionType': '3D', 'MTState': 'false', 'ParallelAcquisitionTechnique': 'GRAPPA'} 

	#The following parameters are specific to either T1 or T2, so they're defined seperatley. After doing this, the dictionary is merged with
	#hardware metadata and fields constant across scans in this category.
	if 'T1' in scan:
		T1_parameters = {'PulseSequenceType': 'MPRAGE', 'EchoTime': 0.00214, 'SpoilingState': 'true', 'SpoilingType': 'RF',
		'EffectiveEchoSpacing': 0.0076,	'InversionTime': 1, 'FlipAngle': 8, 'PhaseEncodingDirection': 'j-', 'B0FieldSource': 'T1w{}_phasediff'.format(run_no)}

		metadata = merge_dicts(hardware_3T, T1_parameters, constants)

	elif 'T2' in scan:
		T2_parameters = {'PulseSequenceType': 'SPACE', 'EchoTime': 0.565, 'EffectiveEchoSpacing': 0.00553, 'PhaseEncodingDirection': 'j-',
		'B0FieldSource': 'T2w{}_phasediff'.format(run_no)}

		metadata = merge_dicts(hardware_3T, T2_parameters, constants)

	#The metadata file is created using the above information. Keys are sorted alphabetically.
	with open(metadata_filename, 'w') as f:
		json.dump(dict(sorted(metadata.items())), f)

#Defines 3T slice time data (ms), to be used in the function below.
slice_time_ms_3T = [0.0, 390.0, 77.5, 467.5, 157.5, 545.0, 235.0, 622.5, 312.5, 0.0, 390.0, 77.5, 467.5,
157.5, 545.0, 235.0, 622.5, 312.5, 0.0, 390.0, 77.5, 467.5, 157.5, 545.0, 235.0, 622.5, 312.5, 0.0, 390.0,
77.5, 467.5, 157.5, 545.0, 235.0, 622.5, 312.5, 0.0, 390.0, 77.5, 467.5, 157.5, 545.0, 235.0, 622.5, 312.5,
0.0, 390.0, 77.5, 467.5, 157.5, 545.0, 235.0, 622.5, 312.5, 0.0, 390.0, 77.5, 467.5, 157.5, 545.0, 235.0,
622.5, 312.5, 0.0, 390.0, 77.5, 467.5, 157.5, 545.0, 235.0, 622.5, 312.5]

#An empty list is created, and then the 3T slice time values (in ms) are converted to seconds and appended to this list.
slice_time_s_3T = []

for time in slice_time_ms_3T:
	slice_time_s_3T.append(time/1000)

slice_time_s_7T = [0, 0.5325, 0.06, 0.59, 0.12, 0.65, 0.1775, 0.71, 0.2375, 0.7675, 0.295, 0.8275, 0.355, 0.8875,
0.415, 0.945, 0.4725, 0, 0.5325, 0.06, 0.59, 0.12, 0.65, 0.1775, 0.71, 0.2375, 0.7675, 0.295, 0.8275, 0.355, 0.8875,
0.415, 0.945, 0.4725, 0, 0.5325, 0.06, 0.59, 0.12, 0.65, 0.1775, 0.71, 0.2375, 0.7675, 0.295, 0.8275, 0.355, 0.8875,
0.415, 0.945, 0.4725, 0, 0.5325, 0.06, 0.59, 0.12, 0.65, 0.1775, 0.71, 0.2375, 0.7675, 0.295, 0.8275, 0.355, 0.8875,
0.415, 0.945, 0.4725, 0, 0.5325, 0.06, 0.59, 0.12, 0.65, 0.1775, 0.71, 0.2375, 0.7675, 0.295, 0.8275, 0.355, 0.8875,
0.415, 0.945, 0.4725]

#This function creates json metadata files for task runs.
def fmri_metadata():

	#These values apply for all functional scans.
	constants = {'B0FieldSource': '{}{}_bold_fmap'.format(task, direction), 'PhaseEncodingDirection': phase_dirs[session][direction],
		'PulseSequenceType': 'Gradient Echo EPI', 'MRAcquisitionType': '2D', 'MTState': 'false'}

	#The following parameters are specific to either 3T or 7T, so they're defined seperatley. After doing this, the dictionary is merged with
	#hardware metadata and fields constant across scans in this category.
	if session == 'ses-3T':
		parameters_3T = {'NonlinearGradientCorrection': 'false', 'EffectiveEchoSpacing': 0.00058, 'EchoTime': 0.0331,
		'SliceTiming': slice_time_s_3T, 'FlipAngle': 52, 'MultibandAccelerationFactor': 8, 'RepetitionTime': 0.72}
		
		task_data = merge_dicts(hardware_3T, parameters_3T, constants)

	elif session == 'ses-7T':
		
		parameters_7T = {'EffectiveEchoSpacing': 0.00064, 'EchoTime': 0.0222, 'FlipAngle': 45, 'MultibandAccelerationFactor': 5,
		'RepetitionTime': 1, 'SliceTiming': slice_time_s_7T,}
		
		task_data = merge_dicts(hardware_7T, parameters_7T, constants)

	#The following options are dependent on whether the task name ends in a digit (suggests multiple runs). They're added to the above dictionary.
	if task[-1].isdigit():
		metadata_filename = os.path.join(subject_path, 'func', '{}_{}_task-{}_dir-{}_run-{}_bold.json'.format(subject, session, task[:-1], direction, run_no))
		task_data['TaskName'] = task[:-1]

	else:
		metadata_filename = os.path.join(subject_path, 'func', '{}_{}_task-{}_dir-{}_bold.json'.format(subject, session, task, direction))
		task_data['TaskName'] = task

	#The metadata file is created using the above information. Keys are sorted alphabetically.
	with open(metadata_filename, 'w') as f:
		json.dump(dict(sorted(task_data.items())), f)

#This function creates json metadata for difusion scans.
def dwi_metadata():

	#These values apply for all DWI scans.
	constants = {'PulseSequenceType': 'Spin Echo EPI', 'PhaseEncodingDirection': phase_dirs[session][dwi_dir], 'MTState': 'false'}

	#The following parameters are specific to either 3T or 7T, so they're defined seperatley. After doing this, the dictionary is merged with
	#hardware metadata and fields constant across scans in this category.
	if session == 'ses-3T':
		parameters_3T = { 'NonlinearGradientCorrection': 'false', 'EffectiveEchoSpacing': 0.00078,
		'EchoTime': 0.0895, 'FlipAngle': 78, 'MultibandAccelerationFactor': 3, 'RepetitionTime': 5.52, 'TotalReadoutTime': 0.11154}

		dwi_data = merge_dicts(hardware_3T, parameters_3T, constants)

	elif session == 'ses-7T':
		parameters_7T = {'EffectiveEchoSpacing': 0.00082, 'EchoTime': 0.0712, 'FlipAngle': 90, 'MultibandAccelerationFactor': 2,
		'RepetitionTime': 7, 'TotalReadoutTime': 0.16318}

		dwi_data = merge_dicts(hardware_7T, parameters_7T, constants)

	#The name of the output file is defined using relevant paths, set as a json file. This file is then created.
	metadata_filename = os.path.join(subject_path, 'dwi', '{}_{}_acq-dir{}_dir-{}_dwi.json'.format(subject, session, bvalue, dwi_dir))

	#The metadata file is created using the above information. Keys are sorted alphabetically.
	with open(metadata_filename, 'w') as f:
		json.dump(dict(sorted(dwi_data.items())), f)

#This function creates json metadata files for functional fieldmaps.
def fmri_fmap_metadata():

	#These values apply for all functional fieldmaps scans.
	constants = {'B0FieldIdentifier': '{}{}_bold_fmap'.format(task, direction), 'PulseSequenceType': 'Spin Echo Field Map', 'MTState': 'false', 'MRAcquisitionType': '2D'}

	#Iterates over the possible phase encoding directions according to session. This is done because in the case of all functional runs,
	#there are two spin echo field maps with opposite phase encoding directions.
	for fmap_dir in phase_dirs[session]:

		#The following parameters are specific to either 3T or 7T, so they're defined seperatley. After doing this, the dictionary is merged with
		#hardware metadata and fields constant across scans in this category. Phase encoding direction is defined using the identifier of the respective
		#direction, from the dictionary created at the top of this script.
		if session == 'ses-3T':
			parameters_3T = {'NonlinearGradientCorrection': 'false', 'EffectiveEchoSpacing': 0.00058, 'EchoTime': 0.058,
			'FlipAngle': 90, 'MultibandAccelerationFactor': 1, 'RepetitionTime': 7.06, 'TotalReadoutTime': 0.05974, 
			'PhaseEncodingDirection': phase_dirs[session][fmap_dir]}
			
			fmap_data = merge_dicts(hardware_3T, parameters_3T, constants)

		elif session == 'ses-7T':
			parameters_7T = {'EffectiveEchoSpacing': 0.00064, 'EchoTime1': 0.00408, 'EchoTime2': 0.0051, 'FlipAngle': 32, 
			 'RepetitionTime': 0.642, 'TotalReadoutTime': 0.08256, 'PhaseEncodingDirection': phase_dirs[session][fmap_dir]}

			fmap_data = merge_dicts(hardware_7T, parameters_7T, constants)

		#The following options are dependent on whether the task name ends in a digit (suggests multiple runs). They're added to the above dictionary.
		if task[-1].isdigit():
			metadata_filename = os.path.join(subject_path, 'fmap', '{}_{}_acq-{}{}_dir-{}_run-{}_epi.json'.format(subject, session, task[:-1], direction, fmap_dir, run_no))
			fmap_data['IntendedFor'] = '{}/func/{}_{}_task-{}_dir-{}_run-{}_bold.nii.gz'.format(session, subject, session, task[:-1], direction, run_no)
			fmap_data['TaskName'] = task[:-1]

		else:
			metadata_filename = os.path.join(subject_path, 'fmap', '{}_{}_acq-{}{}_dir-{}_epi.json'.format(subject, session, task, direction, fmap_dir))
			fmap_data['IntendedFor'] = '{}/func/{}_{}_task-{}_dir-{}_bold.nii.gz'.format(session, subject, session, task, direction)
			fmap_data['TaskName'] = task

		#The metadata file is created using the above information. Keys are sorted alphabetically.
		with open(metadata_filename, 'w') as f:
			json.dump(dict(sorted(fmap_data.items())), f)

#This function creates json metadata files for the structural fieldmaps.
def structural_fmap_metadata():

	#Relevant fields are defined ina  dictionary. Using the provided input, the structural file this is intended to correspond to is defined.
	parameters = {"IntendedFor": '{}/anat/{}_{}_run-{}_{}.nii.gz'.format(session, subject, session, run_no, scan), 'PulseSequenceType': 'Field Map',
	'EchoTime1': 0.00214, 'EchoTime2': 0.0046, 'NonlinearGradientCorrection': 'false', 'MRAcquisitionType': '2D',
	'MTState': 'false', 'SpoilingState': 'true', 'SpoilingType': 'RF', 'FlipAngle': 50, 'PhaseEncodingDirection': 'i-',
	'B0FieldIdentifier': '{}{}_phasediff'.format(scan, run_no)}

	#The above dictionary is merged with scanner properties.
	fmap_data = merge_dicts(hardware_3T, parameters)

	#The name of the output file is defined using relevant paths, set as a json file. This file is then created.
	metadata_filename = os.path.join(subject_path, 'fmap', '{}_{}_acq-{}_run-{}_phasediff.json'.format(subject, session, scan, run_no))

	#The metadata file is created using the above information. Keys are sorted alphabetically.
	with open(metadata_filename, 'w') as f:
		json.dump(dict(sorted(fmap_data.items())), f)

#This function extracts and outputs the relevant EV information for a given functional run. First, finds the relevant filepath and creates
#an empty dictionary.
def extract_evs():

	EVs_filepath = os.path.join(target_data, 'LINKED_DATA', 'EPRIME', 'EVs')

	data = []

	#This filepath des not exist for some scans. If so, the rest of this fucntion is skipped.
	if not os.path.exists(EVs_filepath):
		return

	#Iterates through each available EV file and defines its name as a variable.
	for ev_file in glob.glob(os.path.join(EVs_filepath, '*.txt')):
		event_name = ev_file[len(EVs_filepath) + 1 : -4]

		#Opens and reads the file, iterates through each row. Values are turned to 'floats'.
		with open(ev_file, 'r') as f:
			for line in f.readlines():
				ev_data = list(map(float, line.split()))

				#Checks whether the 3 necesary rows (onset, duration, weight) are present.
				#If so, a list is added to the dictionary containing three values and the EV name.
				if len(ev_data) != 3:
					continue
				else:
					data.append(ev_data[:2] + [event_name] + ev_data[2:])
	
	#Headers for the output file are defined. Data is then sorted according to the onset time at index 0.
	#This sorted data is written into a tsv file in the func subfolder.
	headers = ['onset', 'duration', 'trial_type', 'value']

	sorted_data = sorted(data, key=operator.itemgetter(0))

	tsv_filename = os.path.join(subject_path, 'func', '{}_{}_task-{}_dir-{}_events.tsv'.format(subject, session, task, direction))

	with open(tsv_filename, 'w', newline = '') as output_file:
		writer = csv.writer(output_file, delimiter = '\t')
		writer.writerow(headers)
		for row in sorted_data:
			writer.writerow(row)				

#Iterates over each subject.
for subject in os.listdir(input_dir):

	#Iterates over each session (3T or 7T)
	for session in os.listdir(os.path.join(input_dir, subject)):

		#Defines a output path for this specific subject, in the output directory.
		subject_path = os.path.join(output_dir, subject, session)
		os.makedirs(subject_path, exist_ok = True)

		#A list of BIDS subfolders. Checks whether each subfolder exists already. If not, its' created. 'anat'
		#subfolder won't be needed for the 7T session.
		if session == 'ses-3T':		
			subfolders = ['anat', 'func', 'fmap', 'dwi']
		elif session == 'ses-7T':
			subfolders = ['func', 'fmap', 'dwi']
		
		for subfolder in subfolders:
			full_path = os.path.join(subject_path, subfolder)
			if os.path.exists(full_path):
				pass
			else:
				os.mkdir(full_path)

		#Iterates over each protocol in the subject's input directory. Each should correspond to a different scan.
		for protocol in os.listdir(os.path.join(input_dir, subject, session)):

			#Defines the relevant directory as a variable.
			target_data = os.path.join(input_dir, subject, session, protocol)

			#Checks if a given folder corresponds to a structural scan. Will catch both instances of a given scan,
			#if more than one exists.
			if 'MPR' in protocol or 'SPC' in protocol:

				#Defines the 'scan' as first three letters of protocol (T1w or T2w)
				scan = protocol[:3]
				run_no = protocol[-1]

				#Converts structural files including, (a) the scan itself, (b) the phase difference field map,
				#and (c) the magnitude field maps. Done using functions defined above.
				BIDS_convert(protocol, 'anat', 'run-{}_{}'.format(run_no, scan))
				BIDS_convert('FieldMap_Phase', 'fmap', 'acq-{}_run-{}_phasediff'.format(scan, run_no))
				magnitude_convert()
				
				#Creates metadata for these files using functions defined above (not needed for magnitude).
				anat_metadata()
				structural_fmap_metadata()
			
			#Finds any functional scans.
			elif 'fMRI' in protocol:

				#Defines task and phase encoding direction as variables. Kept all capitals if for 'WM',
				#otherwise just the first letter of the task is left capitalised.
				if 'WM' in protocol or 'RET' in protocol:
					task = protocol[6:-3]
				else:
					task = protocol[6:-3].lower()
				direction = protocol[-2:]

				#Checks if the task name finishes with a number. If so, there are multiple runs for this task. This changes the ways
				#in which output files are named. Converts functional files including,  (a) the BOLD scan itself, (b) the SBREF image,
				#(c) spin echo field maps. This is done in a loop by iterating over both possible phase encoding directions for this session.
				if task[-1].isdigit():

					run_no = task[-1]

					BIDS_convert(protocol, 'func', 'task-{}_dir-{}_run-{}_bold'.format(task[:-1], direction, run_no))
					BIDS_convert(protocol + '_SBRef', 'func', 'task-{}_dir-{}_run-{}_sbref'.format(task[:-1], direction, run_no))

					for phase_dir in phase_dirs[session]:
						BIDS_convert('SpinEchoFieldMap_{}'.format(phase_dir), 'fmap',  'acq-{}{}_dir-{}_run-{}_epi'.format(task[:-1], direction, phase_dir, run_no))

				#Repeats the covnersions above, but for cases where there are not multiple runs of this task.
				else:

					BIDS_convert(protocol, 'func', 'task-{}_dir-{}_bold'.format(task, direction))
					BIDS_convert(protocol + '_SBRef', 'func', 'task-{}_dir-{}_sbref'.format(task, direction))

					for phase_dir in phase_dirs[session]:
						BIDS_convert('SpinEchoFieldMap_{}'.format(phase_dir), 'fmap',  'acq-{}{}_dir-{}_epi'.format(task, direction, phase_dir))

				#Creates metadata for the BOLD run and for each fieldmap.
				fmri_metadata()
				fmri_fmap_metadata()
				
				#Pulls out the 'events' for this run.
				extract_evs()

			#Finds diffusion scans.
			elif 'Diffusion' in protocol:

				#Diffusion scans are seperated according to 'bvalue' (three for each session). This appropriate range of values is defined.
				bvalues = {'ses-3T': range(95,98), 'ses-7T': range(71,73)}

				#Iterates over the range of bvalues for this session, followed by possible phase encoding direction.
				for bvalue in bvalues[session]:
					for dwi_dir in phase_dirs[session]:

						#Converts necessary files using previous functions including (a) the main DWI nifti file, (b) the corresponding
						#SBRef image, and (c) the bvec and bval files, contained within a single function defined above.
						BIDS_convert('DWI_dir{}_{}'.format(bvalue, dwi_dir), 'dwi', 'acq-dir{}_dir-{}_dwi'.format(bvalue, dwi_dir))
						BIDS_convert('DWI_dir{}_{}_SBRef'.format(bvalue, dwi_dir), 'dwi', 'acq-dir{}_dir-{}_sbref'.format(bvalue, dwi_dir))
						dwi_convert()

						#Metadata for the corresponding DWI file is extracted.
						dwi_metadata()

print("Script has succesfully finished running.")
