#!/bin/bash
set -x

subj_id=2240030
ses_id=V1
data_dir=/tmp/HCP-Dv2_BIDS
data_bucket=s3://HCP-Dv2_BIDS
cpu_usage=8
singularity=`which singularity`
work_dir=/tmp/work
QSIprep_image='/home/umii/hendr522/singularity_images/qsiprep-v0.12.2.sif'
needed_files_folder='/home/faird/shared/code/internal/utilities/pipeline_wrappers/slurm_QSIprep_with_s3_routines_with_ses'

# pull down needed data and files from BIDS bucket
if [ ! -d ${data_dir}/sub-${subj_id}/ses-${ses_id} ]; then
	mkdir -p ${data_dir}/sub-${subj_id}
	s3cmd get ${data_bucket}/sub-${subj_id}/ses-${ses_id} ${data_dir}/sub-${subj_id} --recursive -v
fi
if [ ! -e ${data_dir}/dataset_description.json ]; then
	cp ${needed_files_folder}/dataset_description.json ${data_dir}
fi
if [ ! -e ${data_dir}/participants.tsv ]; then
	s3cmd get ${data_bucket}/participants.tsv ${data_dir} -v 
fi

# create processed and derivatives folders if they do not exist
#if [ ! -d ${data_dir}/processed/QSIprep ]; then
#	mkdir -p ${data_dir}/processed/QSIprep
#fi

if [ ! -d ${data_dir}/derivatives/QSIprep-v0.12.2 ]; then
	mkdir -p ${data_dir}/derivatives/QSIprep-v0.12.2
fi

if [ ! -d ${work_dir} ]; then
	mkdir -p ${work_dir}
fi

singularity run --cleanenv \
    -B ${needed_files_folder}:/mnt \
    -B ${data_dir}:/sngl/data \
    -B ${data_dir}/derivatives/QSIprep-v0.12.2:/sngl/out \
    -B ${work_dir}:/sngl/scratch \
    ${QSIprep_image} \
    /sngl/data /sngl/out participant \
    --fs-license-file /mnt/license.txt \
    --participant-label ${subj_id} \
    --output-resolution 1.7 \
    --unringing-method mrdegibbs \
    --nthreads ${cpu_usage} \
    --omp-nthreads 7 \
    --skip-bids-validation \
    --stop-on-first-crash \
    -v -v -w /sngl/scratch \
    --eddy-config /mnt/eddy_params.json \
    --mem_mb 20000 \
    --low-mem

#push processed outputs to bucket
s3cmd sync -F --recursive -v ${data_dir}/derivatives/QSIprep-v0.12.2/sub-${subj_id}/ses-${ses_id}/ ${data_bucket}/derivatives/QSIprep-v0.12.2/sub-${subj_id}/ses-${ses_id}/

# run filemapper
