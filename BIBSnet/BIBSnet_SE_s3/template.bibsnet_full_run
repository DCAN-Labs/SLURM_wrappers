#!/bin/bash

subj_id=SUBJECTID
ses_id=SESID
data_dir=DATADIR
data_bucket=BUCKET
run_dir=RUNDIR
cpu_usage=8
singularity=`which singularity`

# pull down needed data and files from BIDS bucket
if [ ! -d ${data_dir}/sub-${subj_id}/ses-${ses_id} ]; then
	mkdir -p ${data_dir}/sub-${subj_id}
	s3cmd get ${data_bucket}/sub-${subj_id}/ses-${ses_id} ${data_dir}/sub-${subj_id} --recursive -v
fi
if [ ! -e ${data_dir}/dataset_description.json ]; then
	cp ${run_dir}/dataset_description.json ${data_dir}
fi
if [ ! -e ${data_dir}/participants.tsv ]; then
	s3cmd get ${data_bucket}/participants.tsv ${data_dir} -v #testing
fi

# create processed and derivatives folders if they do not exist
if [ ! -d ${data_dir}/processed/cabinet ]; then
	mkdir -p ${data_dir}/processed/cabinet
fi

# run cabinet 
env -i ${singularity} run \
-B ${data_dir}:/input \
-B ${data_dir}/processed/cabinet:/output \
-B ${run_dir}/license.txt:/opt/freesurfer/license.txt \
/home/faird/shared/code/internal/pipelines/bibsnet_container/bibsnet_v3.0.2.sif \
/input /output participant -jargs /home/cabinet/parameter-file-container.json -start prebibsnet -end postbibsnet -v 

#push processed outputs to bucket
s3cmd sync -F --recursive -v --delete-removed ${data_dir}/processed/cabinet/bibsnet/sub-${subj_id}/ses-${ses_id}/ ${data_bucket}/derivatives/bibsnet/sub-${subj_id}/ses-${ses_id}/








